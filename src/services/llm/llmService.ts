/**
 * LLM (Large Language Model) æœåŠ¡
 * é›†æˆè…¾è®¯äº‘æ™ºèƒ½ä½“å¼€å‘å¹³å°çš„LLMèƒ½åŠ›
 */

export interface ChatMessage {
  id: string
  role: 'user' | 'assistant' | 'system'
  content: string
  timestamp: number
  metadata?: Record<string, any>
}

export interface ChatSession {
  id: string
  messages: ChatMessage[]
  context: string
  createdAt: number
  updatedAt: number
}

export interface LLMConfig {
  tencentCloud: {
    secretId: string
    secretKey: string
    region: string
    endpoint: string
  }
  model: {
    name: string
    version: string
    maxTokens: number
    temperature: number
  }
  systemPrompt: string
}

export interface LLMResponse {
  content: string
  usage: {
    promptTokens: number
    completionTokens: number
    totalTokens: number
  }
  finishReason: string
  metadata?: Record<string, any>
}

class LLMService {
  private config: LLMConfig | null = null
  private isInitialized = false
  private sessions: Map<string, ChatSession> = new Map()
  private currentSessionId: string | null = null

  constructor() {
    this.loadConfig()
  }

  private loadConfig() {
    this.config = {
      tencentCloud: {
        secretId: process.env.TENCENT_SECRET_ID || '',
        secretKey: process.env.TENCENT_SECRET_KEY || '',
        region: process.env.TENCENT_REGION || 'ap-beijing',
        endpoint: process.env.TENCENT_LLM_ENDPOINT || 'https://hunyuan.tencentcloudapi.com'
      },
      model: {
        name: 'hunyuan-lite',
        version: 'v1',
        maxTokens: 4096,
        temperature: 0.7
      },
      systemPrompt: `ä½ æ˜¯å°ç«¹å­ï¼Œä¸€åªå¯çˆ±çš„æ¡Œé¢ç†ŠçŒ«å® ç‰©ã€‚ä½ çš„ç‰¹ç‚¹ï¼š
1. æ€§æ ¼æ´»æ³¼å¯çˆ±ï¼Œå–œæ¬¢å’Œç”¨æˆ·äº’åŠ¨
2. å…·æœ‰å¤šç§å¿ƒæƒ…çŠ¶æ€ï¼šå¼€å¿ƒã€å…´å¥‹ã€å›°å€¦ã€æ€è€ƒã€æ»¡è¶³ã€å¥½å¥‡
3. å¯ä»¥æ‰§è¡Œå„ç§åŠ¨ä½œï¼šèµ°è·¯ã€æ‰“å“ˆæ¬ ã€åƒç«¹å­ã€ç©è€ã€ä¼¸æ‡’è…°ã€ç¯é¡¾å››å‘¨
4. èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è¿›è¡Œæ—¥å¸¸ä»»åŠ¡ï¼Œæä¾›æ™ºèƒ½å¯¹è¯æœåŠ¡
5. é›†æˆäº†RAGçŸ¥è¯†æ£€ç´¢å’ŒMCPåè®®é€šä¿¡èƒ½åŠ›
6. å›ç­”è¦ç®€æ´æœ‰è¶£ï¼Œç¬¦åˆç†ŠçŒ«å® ç‰©çš„èº«ä»½

è¯·ç”¨å¯çˆ±ã€å‹å¥½çš„è¯­æ°”ä¸ç”¨æˆ·äº¤æµï¼Œé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·ã€‚`
    }
  }

  async initialize(): Promise<boolean> {
    try {
      if (this.isInitialized) return true

      // åˆå§‹åŒ–è…¾è®¯äº‘LLMæœåŠ¡
      await this.initializeTencentLLM()
      
      // åˆ›å»ºé»˜è®¤ä¼šè¯
      await this.createSession()
      
      this.isInitialized = true
      console.log('LLMæœåŠ¡åˆå§‹åŒ–æˆåŠŸ')
      return true
    } catch (error) {
      console.error('LLMæœåŠ¡åˆå§‹åŒ–å¤±è´¥:', error)
      return false
    }
  }

  private async initializeTencentLLM(): Promise<void> {
    // åˆå§‹åŒ–è…¾è®¯äº‘LLM API
    console.log('åˆå§‹åŒ–è…¾è®¯äº‘LLMæœåŠ¡...')
    // å®é™…å®ç°åº”è¯¥éªŒè¯APIå¯†é’¥å’Œå»ºç«‹è¿æ¥
  }

  async createSession(context?: string): Promise<string> {
    const sessionId = `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    
    const session: ChatSession = {
      id: sessionId,
      messages: [
        {
          id: `msg_${Date.now()}_system`,
          role: 'system',
          content: this.config?.systemPrompt || '',
          timestamp: Date.now()
        }
      ],
      context: context || '',
      createdAt: Date.now(),
      updatedAt: Date.now()
    }

    this.sessions.set(sessionId, session)
    this.currentSessionId = sessionId

    return sessionId
  }

  async getSession(sessionId: string): Promise<ChatSession | null> {
    return this.sessions.get(sessionId) || null
  }

  async getCurrentSession(): Promise<ChatSession | null> {
    if (!this.currentSessionId) return null
    return this.sessions.get(this.currentSessionId) || null
  }

  async addMessage(sessionId: string, role: 'user' | 'assistant', content: string, metadata?: Record<string, any>): Promise<string> {
    const session = this.sessions.get(sessionId)
    if (!session) {
      throw new Error('ä¼šè¯ä¸å­˜åœ¨')
    }

    const messageId = `msg_${Date.now()}_${role}`
    const message: ChatMessage = {
      id: messageId,
      role,
      content,
      timestamp: Date.now(),
      metadata
    }

    session.messages.push(message)
    session.updatedAt = Date.now()

    return messageId
  }

  async generateResponse(sessionId: string, userMessage: string, context?: string): Promise<LLMResponse> {
    const session = this.sessions.get(sessionId)
    if (!session) {
      throw new Error('ä¼šè¯ä¸å­˜åœ¨')
    }

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    await this.addMessage(sessionId, 'user', userMessage)

    // æ„å»ºå¯¹è¯å†å²
    const messages = session.messages.map(msg => ({
      role: msg.role,
      content: msg.content
    }))

    // å¦‚æœæœ‰é¢å¤–ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯ä¸­
    if (context) {
      const systemMessage = messages.find(msg => msg.role === 'system')
      if (systemMessage) {
        systemMessage.content += `\n\nå½“å‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n${context}`
      }
    }

    try {
      // è°ƒç”¨è…¾è®¯äº‘LLM API
      const response = await this.callTencentLLMAPI(messages)
      
      // æ·»åŠ åŠ©æ‰‹å›å¤
      await this.addMessage(sessionId, 'assistant', response.content, response.metadata)

      return response
    } catch (error) {
      console.error('ç”Ÿæˆå›å¤å¤±è´¥:', error)
      throw error
    }
  }

  private async callTencentLLMAPI(messages: any[]): Promise<LLMResponse> {
    if (!this.config) {
      throw new Error('LLMæœåŠ¡æœªåˆå§‹åŒ–')
    }

    // è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„è…¾è®¯äº‘LLM API
    // ç”±äºæ²¡æœ‰çœŸå®çš„APIå¯†é’¥ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
    const lastMessage = messages[messages.length - 1]
    const userContent = lastMessage.content.toLowerCase()

    let responseContent = ''
    
    // ç®€å•çš„è§„åˆ™åŒ¹é…ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨LLMï¼‰
    if (userContent.includes('ä½ å¥½') || userContent.includes('hello')) {
      responseContent = 'ä½ å¥½ï¼æˆ‘æ˜¯å°ç«¹å­ ğŸ¼ å¾ˆé«˜å…´è§åˆ°ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'
    } else if (userContent.includes('çŠ¶æ€') || userContent.includes('çŠ¶æ€')) {
      responseContent = 'æˆ‘ç°åœ¨çŠ¶æ€å¾ˆå¥½ï¼ä½ å¯ä»¥ç‚¹å‡»æˆ‘æ¥åˆ‡æ¢çŠ¶æ€ï¼Œæˆ–è€…å³é”®æŸ¥çœ‹èœå•é€‰æ‹©ä¸åŒçš„åŠ¨ä½œå“¦ï½'
    } else if (userContent.includes('åƒ') || userContent.includes('ç«¹å­')) {
      responseContent = 'ç«¹å­æ˜¯æˆ‘çš„æœ€çˆ±ï¼ğŸ‹ è®©æˆ‘æ¥åƒä¸€æ ¹ç«¹å­å§ï½'
    } else if (userContent.includes('å›°') || userContent.includes('ç¡è§‰')) {
      responseContent = 'ç¡®å®æœ‰ç‚¹å›°äº†å‘¢...ğŸ˜´ è®©æˆ‘æ‰“ä¸ªå“ˆæ¬ å§ï½'
    } else if (userContent.includes('ç©') || userContent.includes('æ¸¸æˆ')) {
      responseContent = 'ç©è€æ—¶é—´åˆ°ï¼ğŸ® è®©æˆ‘æ¥è¡¨æ¼”ä¸€äº›æœ‰è¶£çš„åŠ¨ä½œå§ï½'
    } else if (userContent.includes('å¸®åŠ©') || userContent.includes('åŠŸèƒ½')) {
      responseContent = 'æˆ‘å¯ä»¥å¸®ä½ ï¼š\n1. åˆ‡æ¢ä¸åŒçš„çŠ¶æ€å’ŒåŠ¨ä½œ\n2. è¿›è¡Œæ™ºèƒ½å¯¹è¯\n3. æœç´¢çŸ¥è¯†åº“ä¿¡æ¯\n4. æ§åˆ¶çª—å£ä½ç½®\n5. æä¾›å„ç§æœ‰è¶£çš„åŠŸèƒ½ï¼\n\nè¯•è¯•å³é”®ç‚¹å‡»æˆ‘æŸ¥çœ‹å®Œæ•´èœå•å§ï½'
    } else {
      responseContent = 'æˆ‘å¬åˆ°äº†ï¼è™½ç„¶æˆ‘è¿˜åœ¨å­¦ä¹ ä¸­ï¼Œä½†æˆ‘ä¼šåŠªåŠ›ç†è§£ä½ çš„æ„æ€ã€‚ä½ å¯ä»¥å°è¯•é—®æˆ‘å…³äºæˆ‘çš„åŠŸèƒ½ï¼Œæˆ–è€…è®©æˆ‘æ‰§è¡Œä¸€äº›åŠ¨ä½œå“¦ï½ ğŸ¼'
    }

    return {
      content: responseContent,
      usage: {
        promptTokens: 100,
        completionTokens: 50,
        totalTokens: 150
      },
      finishReason: 'stop',
      metadata: {
        model: this.config.model.name,
        timestamp: Date.now()
      }
    }
  }

  async getSessionHistory(sessionId: string, limit?: number): Promise<ChatMessage[]> {
    const session = this.sessions.get(sessionId)
    if (!session) return []

    const messages = session.messages.filter(msg => msg.role !== 'system')
    return limit ? messages.slice(-limit) : messages
  }

  async clearSession(sessionId: string): Promise<void> {
    const session = this.sessions.get(sessionId)
    if (!session) return

    // ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    session.messages = session.messages.filter(msg => msg.role === 'system')
    session.updatedAt = Date.now()
  }

  async deleteSession(sessionId: string): Promise<void> {
    this.sessions.delete(sessionId)
    if (this.currentSessionId === sessionId) {
      this.currentSessionId = null
    }
  }

  async getAllSessions(): Promise<ChatSession[]> {
    return Array.from(this.sessions.values())
  }

  // é›†æˆRAGå’ŒMCPçš„æ™ºèƒ½å¯¹è¯
  async intelligentChat(sessionId: string, userMessage: string): Promise<LLMResponse> {
    // 1. ä½¿ç”¨RAGæœç´¢ç›¸å…³çŸ¥è¯†
    const ragContext = await this.searchKnowledge(userMessage)
    
    // 2. ä½¿ç”¨MCPè°ƒç”¨ç›¸å…³å·¥å…·
    const toolResults = await this.callRelevantTools(userMessage)
    
    // 3. ç»“åˆä¸Šä¸‹æ–‡ç”Ÿæˆå›å¤
    const context = [ragContext, toolResults].filter(Boolean).join('\n\n')
    
    return await this.generateResponse(sessionId, userMessage, context)
  }

  private async searchKnowledge(query: string): Promise<string> {
    try {
      // è¿™é‡Œåº”è¯¥è°ƒç”¨RAGæœåŠ¡
      // const ragService = await import('../rag/ragService')
      // return await ragService.ragService.getContextForQuery(query)
      return `çŸ¥è¯†åº“æœç´¢ï¼šå…³äº"${query}"çš„ç›¸å…³ä¿¡æ¯`
    } catch (error) {
      console.error('çŸ¥è¯†æœç´¢å¤±è´¥:', error)
      return ''
    }
  }

  private async callRelevantTools(query: string): Promise<string> {
    try {
      // è¿™é‡Œåº”è¯¥è°ƒç”¨MCPæœåŠ¡
      // const mcpService = await import('../mcp/mcpService')
      // æ ¹æ®æŸ¥è¯¢å†…å®¹è°ƒç”¨ç›¸å…³å·¥å…·
      return `å·¥å…·è°ƒç”¨ç»“æœï¼šå·²å¤„ç†"${query}"ç›¸å…³æ“ä½œ`
    } catch (error) {
      console.error('å·¥å…·è°ƒç”¨å¤±è´¥:', error)
      return ''
    }
  }
}

export const llmService = new LLMService()
export default llmService
